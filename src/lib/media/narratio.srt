1
00:00:00,233 --> 00:00:03,466
 This video provides a guided tour through the archive of

2
00:00:03,500 --> 00:00:09,099
 Ecologies of LLM Practices, EL2MP Project, which examines

3
00:00:09,133 --> 00:00:12,400
 how skilled professionals utilize and integrate large

4
00:00:12,433 --> 00:00:15,566
 language models, LLMs, into their daily work.

5
00:00:16,266 --> 00:00:20,333
 Rather than isolating technical capabilities, EL2MP

6
00:00:20,366 --> 00:00:24,199
 situates LLMs within broader professional ecologies,

7
00:00:24,766 --> 00:00:27,800
 revealing how they reshape labor through tasks like prompt

8
00:00:27,833 --> 00:00:30,433
 crafting, error evaluation, and the

9
00:00:30,466 --> 00:00:32,899
 discretization of continuous workflows.

10
00:00:34,166 --> 00:00:37,733
 The archive traces the evolving relationship between users

11
00:00:37,766 --> 00:00:41,700
 and LLMs through a series of scenes, each depicting a

12
00:00:41,733 --> 00:00:44,100
 distinct mode of engagement between

13
00:00:44,133 --> 00:00:46,466
 generative AI and professional practice.

14
00:00:47,433 --> 00:00:50,700
 Instead of assessing efficiency or outcomes, Keddingham

15
00:00:50,733 --> 00:00:55,000
 explores how LLMs reconfigure time, space, and cognitive

16
00:00:55,033 --> 00:00:58,966
 effort, how they must be domesticated to fit into existing

17
00:00:59,000 --> 00:01:00,966
 routines, or how they sometimes

18
00:01:01,000 --> 00:01:03,066
 resist integration altogether.

19
00:01:03,933 --> 00:01:07,400
 It examines how these systems compress, fragment, or

20
00:01:07,433 --> 00:01:09,333
 inflate different dimensions of work.

21
00:01:10,633 --> 00:01:14,233
 Rejecting grand theories or predictive models, Keddingham

22
00:01:14,266 --> 00:01:18,033
 offers a grounded chronicle of working with and around

23
00:01:18,066 --> 00:01:23,299
 LLMs, recursive, opaque, laborious, and often tedious.

24
00:01:24,233 --> 00:01:28,000
 It probes the performative force of AI hype as it manifests

25
00:01:28,033 --> 00:01:30,766
 in the rhythms of everyday professional life.

26
00:01:32,533 --> 00:01:34,833
 September 2024 Expectations.

27
00:01:36,133 --> 00:01:38,933
 They met for the first time in late September 2024.

28
00:01:39,733 --> 00:01:42,833
 Our first co-inquires gathered around the table, as they

29
00:01:42,866 --> 00:01:45,233
 would do each Monday for the next six months.

30
00:01:46,266 --> 00:01:48,866
 Agnes, from a literary background and currently a student

31
00:01:48,900 --> 00:01:51,133
 in international relations, remains

32
00:01:51,166 --> 00:01:53,433
 deeply skeptical of large language models.

33
00:01:54,366 --> 00:01:57,266
 One lives perfectly well without technology, she insists.

34
00:01:58,133 --> 00:02:01,900
 Yet she has chosen to take a closer look, cautiously and on

35
00:02:01,933 --> 00:02:03,633
 her terms, drawing from her

36
00:02:03,666 --> 00:02:06,033
 experience in diplomacy and policy writing.

37
00:02:06,966 --> 00:02:10,099
 To her rate, Constance listens quietly.

38
00:02:10,666 --> 00:02:13,699
 An economist by training, she previously worked as a

39
00:02:13,733 --> 00:02:16,466
 research assistant for a renowned French economist, where

40
00:02:16,566 --> 00:02:19,833
 she performed data cleaning and explored datasets.

41
00:02:21,099 --> 00:02:24,233
 These are tasks she believes could plausibly be automated,

42
00:02:24,533 --> 00:02:27,533
 and in some ways she hopes they will be, so that it could

43
00:02:27,566 --> 00:02:29,466
 open space for more meaningful work.

44
00:02:30,533 --> 00:02:32,333
 She uses two chat GBT accounts,

45
00:02:32,766 --> 00:02:34,566
 one personal and one institutional.

46
00:02:37,033 --> 00:02:39,866
 Two seeds town, Kami leans forward slightly.

47
00:02:40,533 --> 00:02:43,433
 She's becoming a lawyer and an early adopter of juris

48
00:02:43,466 --> 00:02:45,533
 logic, a paid legal drafting

49
00:02:45,566 --> 00:02:47,633
 service, during her early studies.

50
00:02:48,533 --> 00:02:50,733
 "It's just more efficient," she explains.

51
00:02:52,033 --> 00:02:55,099
 She's experimented with chat GBT outside the classroom for

52
00:02:55,133 --> 00:02:59,033
 creating hiking itineraries, recipes, and casual planning,

53
00:02:59,233 --> 00:03:01,199
 and it is aware of its limitations.

54
00:03:02,166 --> 00:03:05,033
 For academic work, honestly, it's not that useful.

55
00:03:06,333 --> 00:03:07,500
 Next to her, Alice sits

56
00:03:07,533 --> 00:03:09,266
 upright in a neatly tailored blazer.

57
00:03:09,833 --> 00:03:13,233
 She hasn't created her own chat GBT account, borrowing

58
00:03:13,266 --> 00:03:15,199
 instead from a friend, but has

59
00:03:15,233 --> 00:03:17,133
 experimented enough to form an opinion.

60
00:03:18,266 --> 00:03:20,133
 "Sometimes it feels too real," she says.

61
00:03:21,066 --> 00:03:23,466
 "I don't like the illusion of talking to a person."

62
00:03:25,033 --> 00:03:27,400
 An occasional user, Alice keeps her distance.

63
00:03:28,066 --> 00:03:29,633
 Her real passion lies elsewhere.

64
00:03:30,099 --> 00:03:33,500
 She hopes to become an auctioneer, an interest sparked by

65
00:03:33,533 --> 00:03:35,266
 her parents' passion for antiques.

66
00:03:35,766 --> 00:03:40,000
 The End Towards the end of the table, Guillain mourns

67
00:03:40,033 --> 00:03:43,133
 mostly silent, observing with the preserved attentiveness.

68
00:03:43,833 --> 00:03:46,699
 A student in public policy with a focus on technology

69
00:03:46,733 --> 00:03:50,466
 regulation, his beliefs echo long-termist framings

70
00:03:50,500 --> 00:03:52,566
 popularized by institutions like

71
00:03:52,599 --> 00:03:54,300
 the Future of Humanity Institute.

72
00:03:55,333 --> 00:03:59,333
 Generative AI, he suggests, could change the future of

73
00:03:59,366 --> 00:04:02,900
 intelligence itself, a prospect he finds deeply concerning

74
00:04:02,933 --> 00:04:04,633
 as much as it is unstoppable.

75
00:04:06,633 --> 00:04:09,466
 Across from him, Tobias scrolls on his laptop screen.

76
00:04:10,533 --> 00:04:13,800
 Another student training in digital public policy once used

77
00:04:13,833 --> 00:04:17,333
 chat GBT to write a Python crawler for YouTube, and

78
00:04:17,366 --> 00:04:19,566
 describes LLMs as amplifiers.

79
00:04:20,666 --> 00:04:22,300
 Powerful if you already know what you're doing.

80
00:04:23,166 --> 00:04:26,366
 He values them not for their autonomy, but for their

81
00:04:26,399 --> 00:04:28,899
 ability to extend existing technical skills.

82
00:04:30,066 --> 00:04:33,433
 He speaks with reserved precision, approaching AI from a

83
00:04:33,466 --> 00:04:36,733
 rational perspective, weighing arguments and seeking

84
00:04:36,766 --> 00:04:38,466
 evidence to support his claims.

85
00:04:42,333 --> 00:04:45,566
 Guillain sits with one leg tucked beneath him, a loose

86
00:04:45,600 --> 00:04:47,899
 strand from his guts of death falling over his shoulder.

87
00:04:49,066 --> 00:04:52,033
 The geekiest of the group, he brings a chemistry background

88
00:04:52,066 --> 00:04:55,533
 into his current training in environmental urbanism.

89
00:04:56,766 --> 00:05:01,300
 He approaches AI with a tinkerer's ethic, informed by a

90
00:05:01,333 --> 00:05:05,500
 faintly anti-capitalist sensibility, consistently seeking

91
00:05:05,533 --> 00:05:07,733
 ways to subvert its intended uses.

92
00:05:09,066 --> 00:05:12,866
 His curiosity is practical, and he is open about his

93
00:05:12,899 --> 00:05:15,166
 emotional responses technology provokes.

94
00:05:16,833 --> 00:05:18,933
 Moments of joy, but also frustration.

95
00:05:20,133 --> 00:05:22,133
 He fills the room with his humorous presence.

96
00:05:24,366 --> 00:05:27,199
 A few seats away, Charlotte's leather jacket rests on the

97
00:05:27,233 --> 00:05:29,533
 back of her chair, its patches visible in

98
00:05:29,566 --> 00:05:32,733
 the folds, among them a pro-Palestine patch.

99
00:05:35,199 --> 00:05:38,100
 Trained in human rights law and shaped by her up-reading in

100
00:05:38,133 --> 00:05:41,366
 East German city, known for its polarized politics around

101
00:05:41,399 --> 00:05:43,533
 migration, she has developed a

102
00:05:43,566 --> 00:05:45,466
 lasting interest in migrants' rights.

103
00:05:46,433 --> 00:05:48,666
 Her focus lies in the humanitarian field.

104
00:05:49,633 --> 00:05:53,100
 Machine translation, she says, could be a lifeline in

105
00:05:53,133 --> 00:05:56,500
 refugee contexts, but no algorithm, in

106
00:05:56,533 --> 00:05:59,166
 her view, should replace moral discernment.

107
00:05:59,233 --> 00:06:07,433
 Despite their varied backgrounds, beliefs, and prior

108
00:06:07,466 --> 00:06:09,933
 experiences with LLMs, the group

109
00:06:09,966 --> 00:06:11,866
 shared two recurring expectations.

110
00:06:13,033 --> 00:06:16,566
 That these tools could help them save time and alleviate

111
00:06:16,600 --> 00:06:19,166
 the repetitive, low-value tasks that

112
00:06:19,199 --> 00:06:20,866
 clutter their professional routines.

113
00:06:21,966 --> 00:06:23,633
 For some, that meant automating data

114
00:06:23,666 --> 00:06:26,733
 cleaning or formatting equations in latex.

115
00:06:27,666 --> 00:06:30,899
 For others, it was about drafting administrative emails or

116
00:06:30,933 --> 00:06:33,199
 translating texts more efficiently.

117
00:06:36,666 --> 00:06:40,766
 At the same time, many felt like relative novices,

118
00:06:41,333 --> 00:06:44,600
 uncertain about how to use these tools effectively, yet

119
00:06:44,633 --> 00:06:46,000
 eager to test the capabilities

120
00:06:46,033 --> 00:06:47,899
 they had heard so much about.

121
00:06:48,366 --> 00:06:51,133
 October 2024, discretization.

122
00:06:51,933 --> 00:06:54,766
 From the onset of the protocol, we asked participants,

123
00:06:55,366 --> 00:06:57,366
 "What tasks do you use with LLMs?

124
00:06:57,800 --> 00:06:59,199
 What else could you use them for?"

125
00:06:59,433 --> 00:07:02,033
 Most first turned to clearly defined tasks.

126
00:07:02,566 --> 00:07:05,266
 Interestingly, they had already been using these services,

127
00:07:05,633 --> 00:07:08,966
 Google and Wikipedia for research, DeepL for translation,

128
00:07:09,699 --> 00:07:12,333
 Stack Overflow and documentation for help with coding,

129
00:07:12,800 --> 00:07:16,300
 chatgbt replaced them all, akin to a convenience store.

130
00:07:17,266 --> 00:07:19,600
 Participants appreciated the time it saved and the comfort

131
00:07:19,633 --> 00:07:22,399
 of staying within a one-size-fits-all platform.

132
00:07:23,066 --> 00:07:25,566
 But not all uses were simple substitutions.

133
00:07:25,933 --> 00:07:29,066
 Some participants relied on what LLMs were good at,

134
00:07:29,100 --> 00:07:30,466
 depending on what they picked up

135
00:07:30,500 --> 00:07:32,333
 from the media, friends, or colleagues.

136
00:07:33,100 --> 00:07:36,199
 Nearly everyone used chatgbt to paraphrase.

137
00:07:36,766 --> 00:07:40,300
 Using it for coding purposes was also widespread, justified

138
00:07:40,333 --> 00:07:43,366
 by the idea that it's good at it because they'd heard that

139
00:07:43,399 --> 00:07:45,066
 it had been trained on lots of code.

140
00:07:45,733 --> 00:07:48,100
 Yet, for them, coding was mostly seen as

141
00:07:48,133 --> 00:07:50,366
 a means to achieve a more valuable end.

142
00:07:50,833 --> 00:07:52,133
 It didn't need to be elegant,

143
00:07:52,366 --> 00:07:54,033
 it just needed to be functional.

144
00:07:54,833 --> 00:07:56,566
 Constance said she didn't care for

145
00:07:56,600 --> 00:07:59,666
 code was pretty or fast, only then it ran.

146
00:08:00,366 --> 00:08:01,966
 Tobias went further, rejecting the

147
00:08:02,233 --> 00:08:04,766
 idea of "coding as a creative work."

148
00:08:05,600 --> 00:08:07,033
 I think for me it's more like

149
00:08:07,066 --> 00:08:09,366
 implementing something I've already set out.

150
00:08:09,733 --> 00:08:12,600
 I did my research design, so I kind of mapped out the

151
00:08:12,633 --> 00:08:15,233
 process of data collection or data analysis.

152
00:08:15,966 --> 00:08:18,500
 It feels like I've already put the thought into how I

153
00:08:18,533 --> 00:08:21,466
 wanted to do it and then I just tell it to implement.

154
00:08:22,199 --> 00:08:25,199
 Participants also built on the existing division of labor

155
00:08:25,233 --> 00:08:28,833
 in their line of work to assign tasks to the LLM.

156
00:08:29,266 --> 00:08:32,933
 As such, researchers quickly sought to test the accuracy of

157
00:08:32,966 --> 00:08:35,566
 literature views or bibliography management.

158
00:08:36,266 --> 00:08:40,200
 Ichen found chatgbt incredibly efficient, especially in the

159
00:08:40,233 --> 00:08:43,700
 writing of quantitative social science essays since they

160
00:08:43,733 --> 00:08:45,266
 tend to be standard in format.

161
00:08:45,866 --> 00:08:48,733
 At the same time, Charlotte noticed that they're quite good

162
00:08:48,766 --> 00:08:51,333
 when it comes to referencing with rigid guidelines.

163
00:08:51,799 --> 00:08:54,700
 So if there's something really clear like the formula for

164
00:08:54,733 --> 00:08:58,399
 how to do referencing, for example, it usually can do that.

165
00:08:59,000 --> 00:09:02,233
 Most agree that the LLMs performed best on standardized

166
00:09:02,266 --> 00:09:04,700
 tasks governed by precise rules.

167
00:09:05,366 --> 00:09:07,533
 Over time, however, participants began

168
00:09:07,566 --> 00:09:10,033
 to push beyond these obvious use cases.

169
00:09:10,399 --> 00:09:13,500
 In law, for instance, Alice realized what she thought of as

170
00:09:13,533 --> 00:09:16,966
 a single task was too unwieldy for the LLM.

171
00:09:17,266 --> 00:09:20,633
 What felt like one smooth action to her had to be broken

172
00:09:20,666 --> 00:09:23,000
 down into smaller, more manageable steps.

173
00:09:23,766 --> 00:09:27,066
 I think one of the key things to get proper results is

174
00:09:27,100 --> 00:09:29,866
 really to dissect every part of what you

175
00:09:29,899 --> 00:09:32,033
 want to reflect on and go step by step.

176
00:09:32,533 --> 00:09:34,833
 Because even if you give the big picture at the beginning

177
00:09:34,866 --> 00:09:37,000
 and then try to break it down, it

178
00:09:37,033 --> 00:09:39,033
 tends to try to do everything at once.

179
00:09:39,600 --> 00:09:40,866
 I mean, it was a really confused.

180
00:09:41,700 --> 00:09:45,000
 In our last group discussion, Camille suggested using time

181
00:09:45,033 --> 00:09:48,266
 as a rule of thumb to understand what the LLM can handle.

182
00:09:48,666 --> 00:09:51,733
 As a lawyer, she viewed many of her tasks as long term

183
00:09:51,766 --> 00:09:55,600
 processes, cumulative in nature, which didn't translate

184
00:09:55,633 --> 00:09:59,433
 well to the kind of short term local iterative problems

185
00:09:59,466 --> 00:10:01,566
 that LLMs seemed able to tackle.

186
00:10:02,133 --> 00:10:04,433
 LLMs have difficulty following a

187
00:10:04,466 --> 00:10:06,966
 progressive and logical approach over the long term.

188
00:10:07,433 --> 00:10:10,833
 At first, I thought, OK, law mostly relies on logic and

189
00:10:10,866 --> 00:10:13,366
 even on reasoning that's very close to mathematics.

190
00:10:13,933 --> 00:10:15,533
 So LLMs could be really good at it.

191
00:10:15,833 --> 00:10:18,733
 But working with them showed me that it's often very hard

192
00:10:18,766 --> 00:10:22,066
 for the machine to build on previous versions, to integrate

193
00:10:22,100 --> 00:10:25,233
 feedback, and since my work required that kind of

194
00:10:25,266 --> 00:10:28,633
 continuity, sometimes the long process became the most

195
00:10:28,666 --> 00:10:30,799
 frustrating aspect of the project.

196
00:10:31,333 --> 00:10:33,366
 That's a more global kind of reasoning.

197
00:10:33,899 --> 00:10:36,333
 And I think the LLM doesn't really do that because it

198
00:10:36,366 --> 00:10:39,000
 responds to specific prompts and tasks.

199
00:10:39,433 --> 00:10:40,799
 So when at the end you ask for

200
00:10:40,833 --> 00:10:42,833
 a global answer, it struggles.

201
00:10:43,533 --> 00:10:46,533
 Although this was often disappointing, it also reminded

202
00:10:46,566 --> 00:10:49,633
 participants that their jobs weren't so easy to automate.

203
00:10:50,100 --> 00:10:52,933
 "I thought my work would be simple," Guillaume reflected,

204
00:10:53,533 --> 00:10:55,000
 something so standardized that

205
00:10:55,033 --> 00:10:56,366
 the LLM would handle it quickly.

206
00:10:56,966 --> 00:10:58,433
 But actually it wasn't.

207
00:10:58,833 --> 00:11:00,666
 The next week, Camille echoed him.

208
00:11:01,033 --> 00:11:04,200
 "I think it shows that law, even when it looks like a

209
00:11:04,233 --> 00:11:08,133
 simple legal task, always contains hidden complexity."

210
00:11:08,733 --> 00:11:10,233
 Listening to Camille, Charlotte

211
00:11:10,266 --> 00:11:12,799
 introduced a metaphor that resonated with many.

212
00:11:13,299 --> 00:11:15,333
 Working with an LLM felt like trying

213
00:11:15,366 --> 00:11:17,733
 to fit your job into a small window.

214
00:11:18,266 --> 00:11:19,700
 Even the act of framing the

215
00:11:19,733 --> 00:11:21,233
 task could feel like a struggle.

216
00:11:21,866 --> 00:11:24,266
 Yeah, I think, do you ever feel that too?

217
00:11:24,966 --> 00:11:26,433
 Because like keep struggling with it.

218
00:11:26,733 --> 00:11:28,633
 Like the window to put in the

219
00:11:28,666 --> 00:11:31,933
 information in all the context just feels so small.

220
00:11:32,600 --> 00:11:35,000
 Like sometimes you don't even know how to fit everything

221
00:11:35,033 --> 00:11:36,433
 in, the struggle already

222
00:11:36,466 --> 00:11:38,766
 starts with just trying to frame it.

223
00:11:39,433 --> 00:11:41,733
 Fitting information within the window is challenging

224
00:11:41,766 --> 00:11:45,399
 because it wasn't just a matter of providing more context

225
00:11:45,433 --> 00:11:49,133
 to the LLM, because too much context could backfire.

226
00:11:50,133 --> 00:11:51,633
 Constant noticed that overloading the

227
00:11:51,666 --> 00:11:53,966
 prompt creates noise in the response.

228
00:11:54,333 --> 00:11:57,399
 Like, it's farther from what I actually want than if I just

229
00:11:57,433 --> 00:11:59,933
 provide one document with a more precise question.

230
00:12:00,433 --> 00:12:02,633
 This insight led her to change strategy.

231
00:12:03,333 --> 00:12:05,100
 I didn't change the way I prompt,

232
00:12:05,333 --> 00:12:07,733
 but I did change how I use the chats.

233
00:12:08,333 --> 00:12:09,733
 Now I use way more separate

234
00:12:09,766 --> 00:12:12,133
 chats to get more precise responses.

235
00:12:12,533 --> 00:12:13,766
 December 2024.

236
00:12:14,333 --> 00:12:14,933
 Domestication.

237
00:12:16,333 --> 00:12:18,299
 No one enjoyed using LLMs.

238
00:12:18,933 --> 00:12:21,066
 As researchers, this caught us off guard.

239
00:12:21,566 --> 00:12:24,700
 Two months after the protocol began, we had expected at

240
00:12:24,733 --> 00:12:26,833
 least some participants to have fun

241
00:12:26,866 --> 00:12:28,933
 coaxing better answers from the machine.

242
00:12:29,933 --> 00:12:31,033
 Guillaume was the ideal

243
00:12:31,066 --> 00:12:33,033
 candidate who could enjoy the process.

244
00:12:33,700 --> 00:12:36,766
 He had gained a bit of a reputation for how playfully and

245
00:12:36,799 --> 00:12:39,233
 inventively he pushed the machine's limits.

246
00:12:39,966 --> 00:12:43,866
 But even he declared, "No, there's no joy in it, because I

247
00:12:43,899 --> 00:12:45,933
 don't want to prompt the machine to do

248
00:12:45,966 --> 00:12:48,366
 it, and I just want the machine to do it.

249
00:12:48,866 --> 00:12:50,700
 I don't want to have a role in the process.

250
00:12:51,333 --> 00:12:54,366
 My ideal AI would be the one that automatically knows when

251
00:12:54,399 --> 00:12:56,333
 to send an email, sends it,

252
00:12:56,366 --> 00:12:57,766
 and just gets it out of my head."

253
00:12:58,833 --> 00:13:02,433
 Prompting was a chore, something to skip whenever possible.

254
00:13:03,033 --> 00:13:04,833
 The goal was to paste the content,

255
00:13:05,233 --> 00:13:07,766
 press enter, copy the output, and proceed.

256
00:13:08,600 --> 00:13:11,766
 Reaching that point still required some setup, a quick

257
00:13:11,799 --> 00:13:15,166
 prompt at the start of the conversation tailored to a task,

258
00:13:15,200 --> 00:13:17,733
 such as coding, translating, or paraphrasing.

259
00:13:18,733 --> 00:13:21,333
 However, participants rarely bothered with careful

260
00:13:21,366 --> 00:13:23,299
 prompting, unless something went wrong.

261
00:13:24,266 --> 00:13:27,100
 On the topic of getting help with the latex language,

262
00:13:27,633 --> 00:13:31,333
 Planstans confided, "I didn't try to perfect my prompts for

263
00:13:31,366 --> 00:13:33,966
 this type of task, as the responses were generally

264
00:13:34,000 --> 00:13:37,566
 satisfactory, even with very implicit formulations."

265
00:13:38,666 --> 00:13:39,600
 A quick review of their

266
00:13:39,633 --> 00:13:41,433
 prompts confirmed this carelessness.

267
00:13:42,533 --> 00:13:45,200
 Participants acknowledged they were full of typos, hastily

268
00:13:45,233 --> 00:13:47,266
 written, and awkwardly phrased.

269
00:13:48,033 --> 00:13:50,700
 Regarding prompting techniques, many preferred simple

270
00:13:50,733 --> 00:13:53,500
 ritual phrases over complex strategies.

271
00:13:54,266 --> 00:13:57,933
 Tobiis explained, "I especially love the emotional stimuli

272
00:13:57,966 --> 00:14:01,000
 technique, because no matter the prompt, just adding this

273
00:14:01,033 --> 00:14:04,466
 is very important for my work, already leads me to believe

274
00:14:04,500 --> 00:14:07,466
 I put sufficient effort in maxing out the LLM."

275
00:14:08,766 --> 00:14:12,033
 Evaluating the LLM's output posed another challenge.

276
00:14:12,366 --> 00:14:15,466
 As participants encountered more frequent confabulations,

277
00:14:16,033 --> 00:14:18,333
 they grew increasingly wary of their answers.

278
00:14:19,066 --> 00:14:20,833
 While the machine sped up the act of

279
00:14:20,866 --> 00:14:23,633
 writing, it multiplied the work of reading.

280
00:14:24,233 --> 00:14:26,700
 With chatty models like chat GPT, the

281
00:14:26,733 --> 00:14:28,766
 vigilance required became exhausting.

282
00:14:29,633 --> 00:14:32,299
 Many participants worried about the sustainability of

283
00:14:32,333 --> 00:14:35,799
 providing such constant attention, afraid that fatigue or

284
00:14:35,833 --> 00:14:38,233
 ambiguity might wear them down over time.

285
00:14:39,166 --> 00:14:42,733
 Evaluation became even trickier when the LLM convincingly

286
00:14:42,766 --> 00:14:44,366
 mimicked professional voices.

287
00:14:45,233 --> 00:14:49,100
 Camille noted chat GPT's ability to sound loyally, and

288
00:14:49,133 --> 00:14:51,500
 Constance felt it could pass as a typical

289
00:14:51,533 --> 00:14:54,433
 economist, making errors harder to catch.

290
00:14:55,866 --> 00:14:57,166
 Participants coped by turning toward

291
00:14:57,200 --> 00:14:59,500
 tasks that were easier to evaluate.

292
00:15:00,266 --> 00:15:03,399
 Some like Charlotte prefer tasks with a certain baseline of

293
00:15:03,433 --> 00:15:05,133
 reference, in which they had enough

294
00:15:05,166 --> 00:15:07,899
 prior experience to judge the result quickly.

295
00:15:08,733 --> 00:15:12,066
 For others, tasks outside their expertise offered relief,

296
00:15:12,600 --> 00:15:15,333
 either because they didn't care or knew less about them.

297
00:15:16,000 --> 00:15:19,200
 Unburdened by expert knowledge, their evaluation employed

298
00:15:19,233 --> 00:15:20,766
 more straightforward and cruder

299
00:15:20,799 --> 00:15:23,799
 criteria, making it less cognitively demanding.

300
00:15:24,566 --> 00:15:25,933
 Coding was one such task.

301
00:15:26,233 --> 00:15:29,600
 They deemed it mechanical, non-creative, and easy to test,

302
00:15:29,933 --> 00:15:32,033
 as code either runs or doesn't.

303
00:15:32,933 --> 00:15:36,399
 Yet despite efforts to minimize prompting, some tasks

304
00:15:36,433 --> 00:15:38,299
 inevitably required more careful

305
00:15:38,333 --> 00:15:40,466
 crafting or attentive evaluation.

306
00:15:41,299 --> 00:15:44,233
 This usually happened when participants deliberately tested

307
00:15:44,266 --> 00:15:47,500
 the LLM's limits, or believed initial effort would yield

308
00:15:47,533 --> 00:15:49,399
 general prompt setups that could

309
00:15:49,433 --> 00:15:51,233
 be reused in other conversations.

310
00:15:52,366 --> 00:15:55,433
 Agnes discovered early on that crafting precise prompts

311
00:15:55,466 --> 00:15:57,833
 tends to produce better results, making

312
00:15:57,866 --> 00:16:00,266
 the initial effort a worthwhile investment.

313
00:16:00,966 --> 00:16:03,133
 I think I tend to do quite detailed

314
00:16:03,166 --> 00:16:05,733
 prompts because I want the LLM to be effective.

315
00:16:06,433 --> 00:16:09,000
 I really put a lot of information in it.

316
00:16:09,233 --> 00:16:12,333
 When we did the first experiments with this group, I asked

317
00:16:12,366 --> 00:16:13,966
 more general questions and I got a

318
00:16:14,000 --> 00:16:16,066
 lot of hallucinations with chat GPT.

319
00:16:17,933 --> 00:16:21,233
 Towards the end, we explicitly encouraged participants to

320
00:16:21,266 --> 00:16:24,533
 deeper engagement with these challenging tasks to prompt

321
00:16:24,566 --> 00:16:28,333
 participants to pinpoint exactly what made them difficult.

322
00:16:29,366 --> 00:16:32,700
 A recurring challenge was setting the proper context.

323
00:16:33,733 --> 00:16:36,633
 Participants preferred quick, role-play-based prompts, such

324
00:16:36,666 --> 00:16:40,133
 as "you're a social media marketing expert," rather than a

325
00:16:40,166 --> 00:16:43,866
 detailed explanation of what constituted expertise.

326
00:16:44,533 --> 00:16:49,399
 It was easier to make GPT-4 believe it's an expert on the

327
00:16:49,433 --> 00:16:53,399
 topic than doing the work of context setting, which takes a

328
00:16:53,433 --> 00:16:56,766
 lot of energy and thought I'm sometimes too lazy to do.

329
00:16:57,433 --> 00:16:57,666
 Charlotte.

330
00:16:58,866 --> 00:17:00,299
 Another source of difficulty was

331
00:17:00,333 --> 00:17:03,733
 understanding why the LLM didn't perform well.

332
00:17:04,799 --> 00:17:06,466
 Constance describes her frustration.

333
00:17:07,166 --> 00:17:11,033
 "I didn't have the tools to understand the breakdown, and

334
00:17:11,066 --> 00:17:12,633
 so I couldn't solve the problem."

335
00:17:13,433 --> 00:17:17,033
 The core challenge lay in choosing the right words, whether

336
00:17:17,066 --> 00:17:20,599
 it was articulating what felt off in the machine's reply or

337
00:17:20,633 --> 00:17:23,166
 describing precisely what they wanted.

338
00:17:24,500 --> 00:17:26,466
 Participants often felt at a loss for words.

339
00:17:27,166 --> 00:17:29,000
 Even a driven user like Yom

340
00:17:29,033 --> 00:17:31,033
 arrived at a point of wordlessness.

341
00:17:31,666 --> 00:17:34,500
 "I just didn't really know what to say to it anymore."

342
00:17:35,566 --> 00:17:38,400
 Agnes described a similar dead end when she realized she

343
00:17:38,433 --> 00:17:39,700
 didn't know enough about the subject

344
00:17:39,733 --> 00:17:41,666
 matter to address the LLM adequately.

345
00:17:42,533 --> 00:17:45,233
 "When the result is too general, and I don't know enough

346
00:17:45,266 --> 00:17:48,633
 about the subject matter to ask more precise questions, I

347
00:17:48,666 --> 00:17:51,000
 feel like I'm at a dead end because I

348
00:17:51,033 --> 00:17:53,166
 can't choose a new path of questions."

349
00:17:54,533 --> 00:17:58,233
 Trying harder to perfect prompts often intensified

350
00:17:58,333 --> 00:18:02,400
 frustration, especially when participants couldn't gauge if

351
00:18:02,433 --> 00:18:04,633
 their alterations improved results.

352
00:18:05,733 --> 00:18:08,433
 Tobias ultimately determined that prompting techniques were

353
00:18:08,466 --> 00:18:11,900
 almost closer to superstition than engineering.

354
00:18:13,000 --> 00:18:15,533
 While it is difficult to determine whether it actually

355
00:18:15,566 --> 00:18:18,633
 improves the outputs, it leads me to believe I put

356
00:18:18,666 --> 00:18:21,866
 sufficient effort in maxing out the LLM.

357
00:18:22,733 --> 00:18:24,466
 Many felt that the LLM was

358
00:18:24,633 --> 00:18:27,166
 inconsistent in its response to their prompts.

359
00:18:27,966 --> 00:18:30,166
 Agnes shared, "At the beginning, I

360
00:18:30,200 --> 00:18:32,533
 thought I could kind of adjust how it worked.

361
00:18:32,966 --> 00:18:35,633
 But in the end, I found that the more I tried to get a

362
00:18:35,666 --> 00:18:39,799
 precise formulation, the more random the results became.

363
00:18:40,533 --> 00:18:43,333
 I had this experience while trying to get it to use one

364
00:18:43,366 --> 00:18:46,833
 specific word, populism, and the more I

365
00:18:46,866 --> 00:18:50,133
 pushed for that, the weirder the answers got.

366
00:18:51,200 --> 00:18:55,033
 I had no way of knowing how to influence the outcome, so it

367
00:18:55,066 --> 00:18:58,200
 gave me this kind of feeling of absurdity, which was

368
00:18:58,233 --> 00:19:00,633
 surprising because I actually expected to

369
00:19:00,666 --> 00:19:02,533
 be opposite by the end of the experiments."

370
00:19:03,666 --> 00:19:04,666
 Not everyone agreed.

371
00:19:05,099 --> 00:19:06,433
 Some felt they were making progress.

372
00:19:07,099 --> 00:19:10,533
 They could learn, adapt, and sometimes get better results.

373
00:19:11,733 --> 00:19:13,966
 But even those whose results improved

374
00:19:14,000 --> 00:19:16,766
 often found the extra effort unrewarding.

375
00:19:17,233 --> 00:19:19,233
 It caused more than it gave back.

376
00:19:20,133 --> 00:19:23,233
 After a certain point, the time energy invested felt

377
00:19:23,266 --> 00:19:24,866
 disproportionate to the payoff.

378
00:19:25,733 --> 00:19:27,566
 Camille captured this common experience.

379
00:19:28,400 --> 00:19:32,166
 "There's this moment when I realize, after giving multiple

380
00:19:32,200 --> 00:19:37,233
 instructions, clarifying, or rephrasing, that chat.jpt is

381
00:19:37,266 --> 00:19:39,733
 giving me completely off-topic information.

382
00:19:40,733 --> 00:19:42,566
 I end up feeling really frustrated and

383
00:19:42,599 --> 00:19:45,433
 give up out of lack of time and motivation.

384
00:19:46,366 --> 00:19:48,299
 Chat.jpt just doesn't understand

385
00:19:48,333 --> 00:19:50,799
 what I asked despite all my efforts.

386
00:19:51,833 --> 00:19:53,266
 February 2025.

387
00:19:54,500 --> 00:19:54,700
 Aliens."

388
00:19:56,166 --> 00:19:59,066
 As months pass, participants grow more

389
00:19:59,099 --> 00:20:01,966
 confident that chat.jpt won't replace them.

390
00:20:02,866 --> 00:20:04,299
 Some even begin to wonder if

391
00:20:04,333 --> 00:20:06,533
 LLMs were ever genuine contenders.

392
00:20:07,533 --> 00:20:10,533
 At first, many treated the AI like a fellow expert.

393
00:20:11,200 --> 00:20:13,433
 Now they speak to it as if it were an alien.

394
00:20:14,133 --> 00:20:16,066
 Someone unfamiliar with their field.

395
00:20:17,166 --> 00:20:20,033
 Camille realized this shift the day she noticed how similar

396
00:20:20,066 --> 00:20:23,500
 prompting felt to explaining the basics to a clueless and

397
00:20:23,533 --> 00:20:25,299
 stubborn client rather than

398
00:20:25,333 --> 00:20:27,200
 collaborating with a seasoned colleague.

399
00:20:27,933 --> 00:20:30,733
 The most difficult part of

400
00:20:30,766 --> 00:20:32,566
 legal reasoning is reformulating.

401
00:20:33,200 --> 00:20:35,733
 When you have a client coming in with a question that's all

402
00:20:35,766 --> 00:20:38,433
 over the place and you have to figure out what the actual

403
00:20:38,466 --> 00:20:40,966
 problem is and then explain it.

404
00:20:41,466 --> 00:20:43,766
 And with the LLM, it was kind of the same

405
00:20:43,799 --> 00:20:45,966
 because we always had to explain it again.

406
00:20:46,500 --> 00:20:49,033
 You can't really assume that you're talking to a lawyer.

407
00:20:51,166 --> 00:20:53,066
 Their main frustration isn't that the

408
00:20:53,099 --> 00:20:55,533
 AI falls short of professional standards.

409
00:20:56,000 --> 00:20:57,833
 It doesn't seem able to adapt.

410
00:20:58,400 --> 00:21:01,366
 No matter how carefully they prompt or how much context

411
00:21:01,400 --> 00:21:03,833
 they provide, the LLM fails to

412
00:21:03,866 --> 00:21:06,166
 improve or behaves unpredictably.

413
00:21:06,799 --> 00:21:07,900
 During a group discussion,

414
00:21:07,933 --> 00:21:10,333
 Camille compared LLMs to interns.

415
00:21:11,166 --> 00:21:13,733
 Sure, they can handle repetitive tasks like sorting and

416
00:21:13,766 --> 00:21:17,099
 renaming files, but they don't know how to behave.

417
00:21:17,966 --> 00:21:20,099
 Unlike interns, LLMs don't learn

418
00:21:20,133 --> 00:21:21,733
 through context or observation.

419
00:21:22,500 --> 00:21:23,633
 Camille puts it bluntly.

420
00:21:24,333 --> 00:21:25,833
 An intern would revise her

421
00:21:25,866 --> 00:21:27,866
 emails five times without being told.

422
00:21:28,633 --> 00:21:32,466
 An LLM has to be reminded of instructions incessantly.

423
00:21:33,433 --> 00:21:35,433
 Others nodded around the table.

424
00:21:36,200 --> 00:21:37,933
 Agnes recalled her internship at an

425
00:21:37,966 --> 00:21:39,633
 embassy when Charlotte challenged her.

426
00:21:40,599 --> 00:21:42,233
 "Colleagues make mistakes too, so

427
00:21:42,266 --> 00:21:44,200
 how are they different from chat GPT?"

428
00:21:44,633 --> 00:21:45,466
 Agnes replied.

429
00:21:47,433 --> 00:21:50,633
 "I think I distressed the machine more, and maybe I'm just

430
00:21:50,666 --> 00:21:52,533
 biased because it's probabilistic.

431
00:21:52,766 --> 00:21:54,233
 I really don't think it understands.

432
00:21:54,833 --> 00:21:58,299
 But if it's a colleague or an intern, that person can still

433
00:21:58,333 --> 00:22:01,466
 learn, and you can actually teach them how to do it."

434
00:22:02,466 --> 00:22:03,299
 Many agreed.

435
00:22:03,766 --> 00:22:05,599
 It's not the mistakes that bother them.

436
00:22:05,633 --> 00:22:06,566
 It's the lack of learning.

437
00:22:07,466 --> 00:22:09,966
 This disconnect limits the machine's capabilities.

438
00:22:10,766 --> 00:22:12,766
 Yet for some, its very distance from

439
00:22:12,799 --> 00:22:14,799
 their professional world makes it valuable.

440
00:22:15,566 --> 00:22:18,700
 Alice, for instance, appreciates that chat GPT isn't part

441
00:22:18,733 --> 00:22:20,233
 of her social or work circle.

442
00:22:21,000 --> 00:22:22,900
 Not separation creates a space that

443
00:22:22,933 --> 00:22:24,866
 feels private and free of judgment.

444
00:22:25,433 --> 00:22:28,033
 As she once told the group, "It's kind of

445
00:22:28,066 --> 00:22:30,866
 a tool you can use anytime, day or night.

446
00:22:31,266 --> 00:22:34,700
 So you develop a certain kind of interpersonal relationship

447
00:22:34,733 --> 00:22:38,533
 with the LLM, and it feels safe to ask it any question,

448
00:22:39,133 --> 00:22:40,299
 even the kind of question you

449
00:22:40,333 --> 00:22:42,233
 might feel stupid asking someone else.

450
00:22:42,700 --> 00:22:45,166
 You don't feel like you're going to be judged after art,

451
00:22:45,200 --> 00:22:46,566
 even if you say something dumb."

452
00:22:47,566 --> 00:22:51,033
 This type of task was so prevalent and important to

453
00:22:51,066 --> 00:22:53,500
 Charlotte that she referred to it as "silly work."

454
00:22:53,866 --> 00:22:57,633
 In a moment of vulnerability, she confided to the group

455
00:22:57,666 --> 00:23:01,233
 that to calm her anxiety before phone calls, especially in

456
00:23:01,266 --> 00:23:03,700
 English, her second language, she

457
00:23:03,733 --> 00:23:06,033
 would ask chat GPT for a short script.

458
00:23:07,066 --> 00:23:08,533
 She rarely used it, but just

459
00:23:08,566 --> 00:23:10,599
 having it there made her feel prepared.

460
00:23:11,466 --> 00:23:13,966
 The ritual itself mattered more than the result.

461
00:23:14,933 --> 00:23:15,633
 She explained,

462
00:23:16,900 --> 00:23:20,366
 "I always feel a bit weird telling people that I still

463
00:23:20,400 --> 00:23:22,833
 write myself notes before making a phone call.

464
00:23:23,266 --> 00:23:25,599
 It's like, yeah, maybe in the

465
00:23:25,633 --> 00:23:27,666
 professional world there's this kind of judgment.

466
00:23:28,066 --> 00:23:29,966
 Like, you can't even make a call without prepping.

467
00:23:30,566 --> 00:23:33,633
 So that's why it feels lower stakes to do it with something

468
00:23:33,666 --> 00:23:37,066
 like chat GPT than to just do it on my own.

469
00:23:37,666 --> 00:23:40,666
 I could do it myself, but it would take a lot of time.

470
00:23:41,066 --> 00:23:44,633
 And I'd probably feel a bit guilty spending so much time on

471
00:23:44,666 --> 00:23:48,700
 a task that in the end, maybe I don't even need because I

472
00:23:48,733 --> 00:23:50,700
 often don't even look at the notes that much.

473
00:23:51,366 --> 00:23:54,833
 But because it gives me a sense of security and because

474
00:23:54,866 --> 00:23:59,433
 it's fast with chat GPT, it kind of resolves that tension."

475
00:24:01,166 --> 00:24:03,099
 Though Charlotte was the first to label it, we had

476
00:24:03,133 --> 00:24:04,866
 encountered this kind of "silly work"

477
00:24:05,200 --> 00:24:07,933
 long before and others private written logs.

478
00:24:08,900 --> 00:24:12,566
 Constance and Agnes also described turning to chat GPT for

479
00:24:12,599 --> 00:24:15,200
 reassurance, precisely because it

480
00:24:15,233 --> 00:24:17,133
 wasn't part of their social world.

481
00:24:17,700 --> 00:24:20,833
 For Constance, an economist who felt intimidated by

482
00:24:20,866 --> 00:24:23,700
 programming, the LLM offered patient

483
00:24:23,733 --> 00:24:26,000
 support that made her feel capable again.

484
00:24:26,900 --> 00:24:29,166
 For Agnes, the value wasn't in doing

485
00:24:29,200 --> 00:24:32,066
 more or faster, but in feeling at ease.

486
00:24:32,799 --> 00:24:36,766
 You do not go further with the LLM just more serenely.

487
00:24:37,466 --> 00:24:40,233
 April 2025, low pass filter.

488
00:24:41,000 --> 00:24:42,833
 At the end of the first round of collective

489
00:24:42,866 --> 00:24:46,266
 experimentation, we asked our eight participants to reflect

490
00:24:46,299 --> 00:24:48,366
 on their six months of using LLMs.

491
00:24:49,099 --> 00:24:51,133
 What emerged was a story of ambivalence.

492
00:24:51,566 --> 00:24:54,266
 Many spoke of disenchantment, almost literally so.

493
00:24:54,566 --> 00:24:57,233
 The LLM is not magic at both Guillaume and Tobias.

494
00:24:58,166 --> 00:25:01,500
 Others hoped to offload tedious work and save time, but

495
00:25:01,533 --> 00:25:03,033
 half a year later they admitted that

496
00:25:03,066 --> 00:25:05,400
 the LLM didn't live up to that promise.

497
00:25:06,133 --> 00:25:09,099
 The usage hadn't changed much, and yet most had come to

498
00:25:09,133 --> 00:25:11,799
 rely on it for many things and

499
00:25:11,833 --> 00:25:13,466
 confessed they'd feel its absence.

500
00:25:13,966 --> 00:25:16,733
 This admission made some uneasy as Reliance raised

501
00:25:16,866 --> 00:25:18,766
 uncomfortable questions about their

502
00:25:18,799 --> 00:25:21,099
 skills and relationship to their work.

503
00:25:22,266 --> 00:25:25,200
 That discomfort came from a growing sense that LLMs were

504
00:25:25,233 --> 00:25:27,266
 somehow reshaping their work.

505
00:25:27,733 --> 00:25:30,133
 The machine had become a container for the boring,

506
00:25:30,433 --> 00:25:33,333
 uninteresting, uncreative, and unenjoyable.

507
00:25:34,233 --> 00:25:36,733
 Gradually it absorbed every task they didn't like.

508
00:25:37,133 --> 00:25:40,599
 Lumped together in chat GPT, the least fulfilling parts of

509
00:25:40,633 --> 00:25:42,166
 their job became more visible.

510
00:25:43,200 --> 00:25:45,866
 As Charlotte explained, "I think throughout all the

511
00:25:45,900 --> 00:25:48,533
 practice I really found myself gravitating towards some

512
00:25:48,566 --> 00:25:52,633
 boring tasks like we all did, things I don't really enjoy

513
00:25:52,666 --> 00:25:55,933
 and that often take up way too much time, things I don't

514
00:25:55,966 --> 00:25:57,933
 actually want to spend time on.

515
00:25:58,633 --> 00:26:00,366
 For me, that's usually paraphrasing.

516
00:26:01,233 --> 00:26:02,666
 Some didn't feel freed by this.

517
00:26:03,333 --> 00:26:05,566
 The LLM made tedious tasks easier, so

518
00:26:05,599 --> 00:26:07,466
 they found themselves doing more of them.

519
00:26:07,733 --> 00:26:10,599
 Instead of clearing time for creative work, they sank

520
00:26:10,633 --> 00:26:12,633
 deeper into what the tool could handle."

521
00:26:13,533 --> 00:26:15,366
 Agnes, a researcher, noticed this.

522
00:26:16,000 --> 00:26:19,166
 "I'm spending too much time on the literature review part,

523
00:26:19,400 --> 00:26:22,500
 also because of chat GPT, which is pulling in too much

524
00:26:22,533 --> 00:26:24,666
 material, and I should stop that

525
00:26:24,700 --> 00:26:26,799
 and focus more on the thinking part."

526
00:26:27,633 --> 00:26:31,333
 Another researcher, Constance, also worried that LLMs would

527
00:26:31,366 --> 00:26:34,466
 divert economists from the work they should be doing.

528
00:26:36,066 --> 00:26:37,966
 A large part of our time as economists

529
00:26:38,000 --> 00:26:40,900
 is spent on really uninteresting tasks.

530
00:26:41,866 --> 00:26:42,933
 That's probably why economists

531
00:26:42,966 --> 00:26:44,500
 have so many research assistants.

532
00:26:45,799 --> 00:26:48,433
 And also, the profession is relying more and more on heavy

533
00:26:48,466 --> 00:26:52,400
 tools that require tons of annotation, data cleaning, and

534
00:26:52,433 --> 00:26:54,700
 all that, which means less time for

535
00:26:54,733 --> 00:26:57,133
 more theoretical or analytical tasks.

536
00:26:57,766 --> 00:27:00,466
 So in theory, LLMs could help gain time for that.

537
00:27:00,700 --> 00:27:03,366
 But in practice, I don't think that's what's happening.

538
00:27:04,266 --> 00:27:07,166
 Because now that we can do more complex things with

539
00:27:07,200 --> 00:27:10,333
 machines, we end up pushing the tools even further.

540
00:27:10,799 --> 00:27:13,633
 And in the end, I don't think we actually spend more time

541
00:27:13,666 --> 00:27:16,900
 trying to really understand the mechanisms at stake.

542
00:27:18,133 --> 00:27:20,466
 We just want a fancy method that will impress people.

543
00:27:20,799 --> 00:27:22,333
 So it's true, it expands

544
00:27:22,366 --> 00:27:24,133
 possibilities a lot for economists.

545
00:27:24,866 --> 00:27:26,766
 But is that really what we should be doing?

546
00:27:27,500 --> 00:27:28,266
 I don't know.

547
00:27:28,500 --> 00:27:30,733
 Maybe it's not why our discipline is the most valuable.

548
00:27:32,400 --> 00:27:33,833
 LLMs didn't just change what

549
00:27:33,866 --> 00:27:37,033
 participants did, but also how they experienced it.

550
00:27:37,633 --> 00:27:40,200
 As the discussion went on, Guillaume expressed that using

551
00:27:40,233 --> 00:27:43,033
 the LLM for all these tasks made them bland.

552
00:27:43,766 --> 00:27:46,333
 Before using the LLM, I used to do those tasks already.

553
00:27:46,799 --> 00:27:48,866
 I didn't have more or less work.

554
00:27:49,533 --> 00:27:50,633
 It just wasn't boring.

555
00:27:51,533 --> 00:27:54,166
 I mean, I didn't think of it as something super

556
00:27:54,200 --> 00:27:56,466
 interesting, but it wasn't boring either.

557
00:27:57,666 --> 00:27:58,933
 And with the use of the LLM, it

558
00:27:58,966 --> 00:28:00,666
 started to feel more and more boring.

559
00:28:01,466 --> 00:28:04,299
 It's not just that it revealed something about my work, but

560
00:28:04,333 --> 00:28:07,666
 somehow the way we use it creates the boredom.

561
00:28:08,400 --> 00:28:10,900
 Constance, who reached a point where she systematically

562
00:28:10,933 --> 00:28:15,033
 relies on the LLM for all her coding tasks, shared an

563
00:28:15,066 --> 00:28:16,799
 insight similar to Guillaume's.

564
00:28:17,400 --> 00:28:20,200
 While she appreciated the help, she acknowledged that

565
00:28:20,233 --> 00:28:23,466
 getting results is not the same as feeling accomplished.

566
00:28:25,366 --> 00:28:26,633
 Instead of being the source of her

567
00:28:26,666 --> 00:28:29,733
 work, she became a conduit, an interface.

568
00:28:30,633 --> 00:28:32,900
 And I think that's why I have some issues with the work I'm

569
00:28:32,933 --> 00:28:35,633
 producing, because I feel like I'm just an interface for

570
00:28:35,666 --> 00:28:37,066
 code that's already been written.

571
00:28:38,033 --> 00:28:40,200
 But sometimes it's kind of rewarding when you see that

572
00:28:40,233 --> 00:28:41,866
 youth manage to produce the numbers.

573
00:28:42,400 --> 00:28:44,266
 So I'm happy when I get the final results.

574
00:28:44,466 --> 00:28:47,333
 But in the meantime, during those long days of doing it, I

575
00:28:47,366 --> 00:28:49,266
 don't really feel very accomplished, I'd say.

576
00:28:49,766 --> 00:28:52,533
 Whereas I remember when I was starting to code without

577
00:28:52,566 --> 00:28:55,099
 chatgbt, every time I managed to do

578
00:28:55,133 --> 00:28:56,700
 something, it felt like a real event.

579
00:28:58,366 --> 00:29:00,700
 Some worried that this transformation

580
00:29:00,733 --> 00:29:03,099
 had happened gradually, insidiously.

581
00:29:03,833 --> 00:29:05,333
 It crept in unnoticed, and then

582
00:29:05,366 --> 00:29:07,599
 using the LLM became second nature.

583
00:29:08,366 --> 00:29:10,299
 As Tobias put it, "I don't know if

584
00:29:10,333 --> 00:29:11,666
 there's that much emotion involved.

585
00:29:11,966 --> 00:29:13,966
 Honestly, it's more just the matter of habit."

586
00:29:14,966 --> 00:29:17,900
 Months before this discussion, Constance had written in her

587
00:29:17,933 --> 00:29:19,966
 log, "The machine is now part of my

588
00:29:20,000 --> 00:29:21,733
 daily life, whether I use it or not."

589
00:29:22,133 --> 00:29:24,766
 Even unused, the LLM loomed in the background.

590
00:29:25,533 --> 00:29:28,066
 Deciding when to use it became part of the work itself.

591
00:29:29,099 --> 00:29:30,333
 Some came up with discursive

592
00:29:30,366 --> 00:29:32,166
 strategies to justify their use.

593
00:29:33,000 --> 00:29:35,566
 Tobias, for instance, drew a personal line between tasks

594
00:29:35,599 --> 00:29:37,866
 that required agency and those that didn't.

595
00:29:38,599 --> 00:29:40,366
 Others framed it as a matter of self-discipline.

596
00:29:41,266 --> 00:29:43,466
 Guillaume and Agnes, who found themselves in a similar

597
00:29:43,500 --> 00:29:45,766
 challenge, described it as a fight for boundaries.

598
00:29:47,200 --> 00:29:50,033
 Agnes, "I have a very short deadline, like a month and a

599
00:29:50,066 --> 00:29:51,933
 half, and I'm super, super late for work.

600
00:29:52,299 --> 00:29:54,333
 I'm trying not to tell the time pressure gets to me.

601
00:29:54,700 --> 00:29:57,299
 I tell myself, no, I'll take the time I need, even if it

602
00:29:57,333 --> 00:29:58,833
 means some parts are less developed.

603
00:29:59,333 --> 00:30:00,966
 I'm trying not to be a perfectionist,

604
00:30:01,000 --> 00:30:02,866
 trying not to do everything all at once."

605
00:30:03,733 --> 00:30:05,733
 Diem, so you're trying to set boundaries for your work,

606
00:30:05,766 --> 00:30:07,933
 based on what you as a human are capable of doing?

607
00:30:08,966 --> 00:30:12,033
 Agnes, yeah, boundaries for my work, for my topic, and also

608
00:30:12,066 --> 00:30:13,900
 for which tasks I do myself and

609
00:30:13,933 --> 00:30:15,833
 which ones I might do with chat GPT.

610
00:30:16,366 --> 00:30:17,533
 But it's a slippery slope.

611
00:30:17,966 --> 00:30:19,766
 Initially, I didn't want to use it at all.

612
00:30:20,333 --> 00:30:23,133
 However, then I got stuck on some sociological concepts and

613
00:30:23,166 --> 00:30:25,733
 my advisor couldn't help, so I asked GPT.

614
00:30:27,366 --> 00:30:29,866
 And because it worked, I used it to look for literature.

615
00:30:30,766 --> 00:30:33,333
 Since that worked, I would like to use it for a literature

616
00:30:33,366 --> 00:30:35,033
 review, and I'm trying not to.

617
00:30:35,666 --> 00:30:38,333
 But because it works, it's hard not to.

618
00:30:39,033 --> 00:30:40,133
 Still, I don't want to be

619
00:30:40,166 --> 00:30:42,733
 productive just for the sake of productivity.

620
00:30:43,266 --> 00:30:44,733
 I just want to do good work.

